{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Model for QA Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pinecone-client torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hi/Downloads/AiSence/venv/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "your .env file should look like this:\n",
    "\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "PINECONE_API_KEY=your_pinecone_api_key\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone_client = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "\n",
    "# Define index name\n",
    "INDEX_NAME = \"business-qa-bot\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if INDEX_NAME not in pinecone_client.list_indexes():\n",
    "    pinecone_client.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"  # Adjust to your Pinecone environment\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Connect to the index\n",
    "index = pinecone_client.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a business-related dataset\n",
    "# Replace this with a dataset that contains Q&A pairs for the specific business domain\n",
    "\n",
    "data = load_dataset(\"ArunSharmaaaaa/business_for_chatbot\", split=\"train[:1000]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s> [INST] business context, what factors influence work-life integration? [/INST] Hsieh explored diverse interests like playing poker that expanded his business perspectives while maintaining a holistic view on achieving happiness professionally and personally.</s>': '<s> [INST] could expanding habitual spaces increase mentions ? [/INST] Associating a product with a strong existing trigger outside its normal habitat, like pairing a candy with coffee breaks, can increase verbal mentions by linking it to something regularly thought of in a new context.</s>'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents for indexing\n",
    "documents = [\n",
    "    {\"id\": str(i), \n",
    "     \"context\": list(qa.keys())[0].split('[/INST]')[0].replace('<s> [INST] ', '').strip(),  # Extract the question\n",
    "     \"answer\": list(qa.values())[0].split('[/INST]')[1].replace(' [/INST]', '').replace('</s>', '').strip()}  # Extract the answer\n",
    "    for i, qa in enumerate(data)\n",
    "]\n",
    "\n",
    "# Extract only unique contexts for vectorization\n",
    "document_texts = list(set(doc[\"context\"] for doc in documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '0', 'context': 'business context, what factors influence work-life integration?', 'answer': 'Associating a product with a strong existing trigger outside its normal habitat, like pairing a candy with coffee breaks, can increase verbal mentions by linking it to something regularly thought of in a new context.'}, {'id': '1', 'context': 'business context, what factors influence work-life integration?', 'answer': 'For contracts that are valued at $500,000 a federal prime contractor must submit a subcontracting plan that includes a plan for the use of woman- owned businesses.'}, {'id': '2', 'context': 'business context, what factors influence work-life integration?', 'answer': 'Set personal goals, make realistic plans, and lean on advisors.'}, {'id': '3', 'context': 'business context, what factors influence work-life integration?', 'answer': 'Consider emotional/ psychological sides of change.'}, {'id': '4', 'context': 'business context, what factors influence work-life integration?', 'answer': 'Anticipate roadblocks, collaborate on priorities, hold sessions to find solutions.'}]\n"
     ]
    }
   ],
   "source": [
    "# samples\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pre-trained transformer model to create embeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed text\n",
    "\n",
    "def embed_text(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**tokens).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and upsert to Pinecone\n",
    "# 1st approach: Embed the 'context' (question) for each document\n",
    "\n",
    "for doc in documents:\n",
    "    # Assuming you want to embed the 'context' (question)\n",
    "    embedding = embed_text(doc[\"context\"])  # Using 'context' for embedding\n",
    "    index.upsert([(doc[\"id\"], embedding, {\"text\": doc[\"context\"]})])  # Store 'context' in Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd approach: Store Both Context and Answer\n",
    "\n",
    "# Generate embeddings and upsert to Pinecone\n",
    "for doc in documents:\n",
    "    embedding = embed_text(doc[\"context\"])  # Using 'context' for embedding\n",
    "    index.upsert([\n",
    "        (\n",
    "            doc[\"id\"], \n",
    "            embedding, \n",
    "            {\"context\": doc[\"context\"], \"answer\": doc[\"answer\"]}  # Store both context and answer\n",
    "        )\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "\n",
    "# Generate embeddings and batch upsert to Pinecone\n",
    "batch_size = 100  # Adjust based on your dataset size and system capacity\n",
    "batches = [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]\n",
    "\n",
    "for batch in batches:\n",
    "    upserts = []\n",
    "    for doc in batch:\n",
    "        # Ensure 'context' and 'answer' keys exist\n",
    "        context = doc.get(\"context\", \"\")\n",
    "        answer = doc.get(\"answer\", \"\")\n",
    "        \n",
    "        # Embed the context\n",
    "        embedding = embed_text(context)\n",
    "        \n",
    "        # Add to upserts\n",
    "        upserts.append((doc[\"id\"], embedding, {\"context\": context, \"answer\": answer}))\n",
    "    \n",
    "    # Perform batch upsert\n",
    "    index.upsert(upserts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build QA Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 1st approach: Embed the 'context' (question) for each document\n",
    "\n",
    "def answer_query(query):\n",
    "    try:\n",
    "        # Embed the user query\n",
    "        query_embedding = embed_text(query)\n",
    "    \n",
    "        # Search Pinecone for similar contexts\n",
    "        search_results = index.query(\n",
    "            vector=query_embedding, \n",
    "            top_k=3, \n",
    "            include_metadata=True\n",
    "        )\n",
    "    \n",
    "        # Combine retrieved contexts\n",
    "        context = \"\\n\".join([result[\"metadata\"][\"context\"] for result in search_results[\"matches\"]])\n",
    "    \n",
    "        # Generate an answer using OpenAI\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"gpt-4\", \n",
    "            prompt=f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\",\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"I'm sorry, I couldn't process your request at the moment.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2nd approach: Store Both Context and Answer\n",
    "\n",
    "def answer_query(query):\n",
    "    try:\n",
    "        # Embed the user query\n",
    "        query_embedding = embed_text(query)\n",
    "    \n",
    "        # Search Pinecone for similar contexts\n",
    "        search_results = index.query(\n",
    "            vector=query_embedding, \n",
    "            top_k=3, \n",
    "            include_metadata=True\n",
    "        )\n",
    "    \n",
    "        # Check if answers are directly available from Pinecone\n",
    "        retrieved_answers = [result[\"metadata\"].get(\"answer\", \"\") for result in search_results[\"matches\"]]\n",
    "        retrieved_contexts = [result[\"metadata\"].get(\"context\", \"\") for result in search_results[\"matches\"]]\n",
    "        \n",
    "        # Combine retrieved contexts\n",
    "        combined_context = \"\\n\".join(retrieved_contexts)\n",
    "        \n",
    "        # If retrieved answers are sufficient and relevant, use them directly\n",
    "        if all(retrieved_answers):\n",
    "            return \"\\n\".join(retrieved_answers)\n",
    "        \n",
    "        # Otherwise, use OpenAI to generate a response based on the context\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"gpt-4\", \n",
    "            prompt=f\"Context: {combined_context}\\n\\nQuestion: {query}\\nAnswer:\",\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"I'm sorry, I couldn't process your request at the moment.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_query(query_embedding):\n",
    "    try:\n",
    "        # Ensure query_embedding is a 1D list\n",
    "        if isinstance(query_embedding, np.ndarray):\n",
    "            query_embedding = query_embedding.tolist()\n",
    "        elif not isinstance(query_embedding, list):\n",
    "            raise ValueError(\"query_embedding must be a list or numpy array\")\n",
    "\n",
    "        # Query Pinecone\n",
    "        search_results = index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=3,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        # print(\"Search results:\", search_results)\n",
    "        return search_results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Pinecone query: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test QA Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "question: What are the key factors affecting work-life balance?\n",
      "---------------------------------------------\n",
      "response:\n",
      "context: business context, what factors influence work-life integration?\n",
      "answer: Recognizing achievements through celebrations and promotions motivates employees and fuels innovation.\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# single output\n",
    "\n",
    "query = \"What are the key factors affecting work-life balance?\"\n",
    "query_embedding = embed_text(query)\n",
    "\n",
    "search_results = perform_query(query_embedding)\n",
    "\n",
    "if search_results:\n",
    "    best_match = max(search_results[\"matches\"], key=lambda match: match[\"score\"])\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"question: {query}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(\"response:\")\n",
    "    print(f\"context: {best_match['metadata']['context']}\")\n",
    "    print(f\"answer: {best_match['metadata']['answer']}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 96, Score: 0.664621949, Metadata: {'answer': 'Recognizing achievements through celebrations and promotions motivates employees and fuels innovation.', 'context': 'business context, what factors influence work-life integration?'}\n",
      "ID: 94, Score: 0.664621949, Metadata: {'answer': \"C rises force innovation, as Sk ylab's damaged launch did by requiring fast, creative solutions.\", 'context': 'business context, what factors influence work-life integration?'}\n",
      "ID: 95, Score: 0.664621949, Metadata: {'answer': 'When people are mutually accountable for results, there is less room for blame and self-justification.', 'context': 'business context, what factors influence work-life integration?'}\n"
     ]
    }
   ],
   "source": [
    "# get top 3 raw results\n",
    "\n",
    "query = \"What are the key factors affecting work-life balance?\"\n",
    "query_embedding = embed_text(query)\n",
    "\n",
    "search_results = perform_query(query_embedding)\n",
    "if search_results:\n",
    "    for match in search_results[\"matches\"]:\n",
    "        print(f\"ID: {match['id']}, Score: {match['score']}, Metadata: {match['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the index if no longer needed\n",
    "\n",
    "pinecone_client.delete_index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
